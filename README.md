
# NLP Basics and Transformers Repository

Welcome to the NLP Basics and Transformers repository! This repo is having resources for learning the foundational elements of Natural Language Processing (NLP) and understanding the revolutionary impact of transformer models.

## Contents

- **NLP Basics**: Introduction to text preprocessing, tokenization, stemming, and lemmatization.
- **Transformers**: Deep dive into the architecture and applications of transformer models in NLP.

## Definitions

- **Natural Language Processing (NLP)**: A branch of artificial intelligence that helps computers understand, interpret, and manipulate human language.
- **Neural Networks (NNs)**: Computing systems vaguely inspired by the biological neural networks that constitute animal brains.
- **Recurrent Neural Networks (RNNs)**: A class of neural networks where connections between nodes form a directed graph along a temporal sequence.
- **Long Short-Term Memory (LSTM)**: A special kind of RNN, capable of learning long-term dependencies, addressing the vanishing gradient problem.
- **Attention Mechanism**: A process that assigns different weights to different parts of the input, focusing on relevant parts of the data.
- **Transformers**: A type of model that uses self-attention mechanisms to process any part of the input data independently and in parallel.
- **Generative Pre-trained Transformer (GPT)**: An autoregressive language model that uses deep learning to produce human-like text.

## Evolution of NLP Models

- **Neural Networks (Pre-2010s)**: Marked the initial phase in deep learning for NLP, focusing on basic text classification and analysis.
- **Recurrent Neural Networks (Early 2010s)**: Introduced the ability to handle sequences for tasks like language modeling and text generation.
- **Long Short-Term Memory (Mid-2010s)**: LSTMs improved upon RNNs by efficiently learning long-range dependencies in text data.
- **Attention Mechanism (2017)**: The introduction of the attention mechanism allowed models to focus on relevant parts of the input sequence, improving context understanding.
- **"Attention Is All You Need" (2017)**: This seminal paper introduced the transformer model, shifting focus from sequence-based processing to parallelized attention-based processing.
- **Generative Pre-trained Transformer (2018-Present)**: GPT and its successors have set new standards for NLP tasks, leveraging transformers for state-of-the-art performance in language understanding and generation.



